\documentclass[11pt,letterpaper]{article}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}

\renewcommand{\baselinestretch}{1.2}

\pagenumbering{gobble}


\title{{\large SuperUROP 2016-2017 Proposal:}\\
  {\Large Program Induction via Metalinguistic Learning}}
\author{Lucas E. Morales \texttt{\{lucasem\}}}
\date{}


\begin{document}

\maketitle

Programs are first-order descriptions of logical flow to compute output
given valid input. In many circumstances, they contain control flow not
directly relevant to whatever contextual task, but necessary to specify
construction of outputs in the computational system on which they are
syntactically limited. A \emph{metalanguage} provides a higher-order
ecosystem to describe the underlying program structure, with the potential
to convert a particular task from a lower-level primitive manipulation of
input objects to more abstract upper-level meaningful challenges that
prescribe the problems' solutions in a relevant domain \cite{sicp}.

Approaches to program induction as the learning of functional programs in
the form of combinatory lambda calculus have been specified using
hierarchical Bayesian learning techniques \cite{dechter,liang}. These
methods of program induction perform multi-task learning, permitting
inferred programs to be composed of common subprograms. I posit that, for
robust understanding and learning in arbitrary domains, it is not sufficient
for a learning architecture to rely on this memory of subprograms --- that a
sufficiently capable learning structure should support ``jumping out of the
system'' as a means of abstraction by constructing metalanguage where
effective.

Consider the following example of a schematic definition for a formula
\cite{church} which is combinatorially sophisticated in its functional
representation but carries significant meaning in a particular domain:
\[\mathbf{[M-N]} \rightarrow \mathfrak{p} (\lambda\mathbf{a}.
  \texttt{eq} \mathbf{M} \mathbf{[N+a]})1.\]
This not only uses common subprograms, namely the Kleene $\mathfrak{p}$-%
function and the equality propositional function \texttt{eq} (both of which
may use subprograms in their own definitions), but also uses and provides an
effective shorthand relevant for manipulations of positive integer inputs.

Learning mechanisms for metalanguage are well-demonstrated in the Copycat
and Metacat projects \cite{copycat,metacat}. The Copycat project solves a
particular domain of tasks by recognizing structure of rules, in a
first-order representation, to apply them to a broader set of inputs than is
trivially apparent. The authors refer to this as ``mental fluidity.'' The
metalanguage that Copycat uses is able to enact rules of string analogies in
a manner such as \emph{replace the rightmost group into its alphabetic
successor}. The Metacat project expands on this work with ``self-watching''
--- the ability to characterize a system's own processes and recognize
patterns therein, providing more insight in its comparisons.

I intend to formalize a system that, given tasks in a particular domain of
problems, constructs and uses a metalanguage to better solve such tasks. I
will expand on previous works \cite{dechter,liang} for program induction by
introducing a meta-evaluator, designed in part using principles from
the analogy domain \cite{copycat,metacat}, which analyzes solution programs
to find meaningful patterns and construct a metalanguage, which will be
incorporated into the generative mechanisms of program induction as
supplementary to the lower-level combinatorial lambda calculus.  I will
experiment with this system in the context of learning propositional
language (e.g. conjunction, disjunction, ordinal comparison) to solve
certain logical tasks, and may consider other applicable domains for
experimentation.

\begin{thebibliography}{}

\bibitem{sicp}
  Abelson, H., \& Sussman, G. J. (1983).
  Structure and interpretation of computer programs.

\bibitem{church}
  Church, A. (1941)
  \emph{The calculi of lambda-conversion} (No. 6).
  Princeton University Press

\bibitem{dechter}
  Dechter, E., Malmaud, J., Adams, R. P., \& Tenenbaum, J. B. (2013).
  Bootstrap learning via modular concept discovery.
  In \emph{IJCAI}.

\bibitem{copycat}
  Hofstadter, D. R., \& Mitchell, M. (1994).
  The copycat project: A model of mental fluidity and analogy-making.
  \emph{Advances in connectionist and neural computation theory},
  2(31-112), 29-30.

\bibitem{liang}
  Liang, P., Jordan, M. I., \& Klein, D. (2010).
  Learning programs: a hierarchical Bayesian approach.
  \emph{International Conference on Machine Learning}: 639–646

\bibitem{metacat}
  Marshall, J. B. (1999).
  Metacat: a self-watching cognitive architecture for analogy-making
  and high-level perception.
  In \emph{Proceedings of the 24th Annual Conference of the Cognitive
    Science Society.}

%\bibitem{schonfinkel}
%  Sch\"{o}nfinkel, M. (1924).
%  \"{U}ber die bausteine der mathematischen logik.
%  \emph{Mathematische Annalen} (in German) 92: 305—316
%
%\bibitem{curry}
%  Curry, H. B. (1930)
%  Grundlagen der kombinatorischen logik
%  \emph{American Journal of Mathematics} (in German) 52 (3): 509–536.
%
\end{thebibliography}

\end{document}

