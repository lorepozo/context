\documentclass[11pt,letterpaper]{article}
\usepackage[margin=1in]{geometry}

\renewcommand{\baselinestretch}{1.2}

\pagenumbering{gobble}


\title{{\Large Contextual Learning via Integrated Least Effort Knowledge Networks}}
\author{Lucas E. Morales \texttt{\{lucasem\}}}
\date{}


\begin{document}

\maketitle

\section{Introduction}

Knowledge, as a familiarity with things, is attained by experience via
learning. Learning mechanisms both construct and consume knowledge, allowing
for higher order learning compared to the base of sensory input. This
interaction must not be immediately free to use all knowledge: a particular
context must motivate certain atoms of knowledge to be readily available
while setting others further apart, depending on their relationships to
those in contextual proximity.

This project introduces the knowledge network framework, capable of
contextualizing knowledge in an abstract manner. Knowledge can be
represented as distinct atoms of information and their relations in the
structure of a connected network. The network is constructed by preferential
``least effort'' attachment \cite{cancho03}, where the learning of a new
atom of knowledge joins the network with relations to the contextual
knowledge from which it was learned. This network conforms to a mathematical
pattern known as Zipf's Law \cite{zipf49}, where the degree of connectivity
for nodes in the network follow a class of power law distributions
\cite{barabasi99}. A similar pattern arises in social networks, the webgraph
of the Internet, and the statistics of linguistic distribution
\cite{clauset09}.

% Creation of such a system, built on existing learning mechanisms, requires
% the completion of subtasks as follows:
% 
% \begin{itemize}
%   \setlength\itemsep{0em}
%   \item Design the knowledge network to permit a useful interface for a set
%     of sufficient learning mechanisms.
%   \item Augment existing learning mechanisms to utilize contextual
%     knowledge.
%   \item Augment existing learning mechanisms to create and update
%     information in the knowledge network.
%   \item Connect distinct kinds of knowledge generated by multiple learning
%     mechanisms in some manner, at least including temporal locality, and
%     perhaps including meta-interpretive similarities.
% \end{itemize}

\section{Relevant Works}

In the discipline of artificial intelligence, much effort is placed on
designing accurate learning mechanisms. These learning mechanisms are
either designed for a particular domain, or designed to specialize
given consistent domain-specific input.

Special-purpose learning mechanisms include visual object recognition
systems based on the human visual cortex \cite{serre07}, handwritten
character identification and generation \cite{lake15}, visual feature
modification of images \cite{kulkarni15}, and sound texture perception and
synthesis \cite{mcdermott11}.

General-purpose learning mechanisms include hierarchical Bayesian methods
\cite{tenenbaum01}, program learning \cite{liang10}\cite{dechter13},
inductive logic programming \cite{lavrac94}\cite{muggleton15}, and
generative adversarial networks \cite{goodfellow14}.

Each of these mechanisms are remarkable in a specified manner, but lack the
generality that is apparent in human cognition. This is the problem of
\emph{knowledge}, and most learning mechanisms have some internal
representation for it. Those representations are mechanism-specific, and
generally have a flat hierarchy (i.e. where all knowledge is, in some
manner, equidistant). Making all knowledge flat and focused in the single
domain that the learning mechanism is confined in is flawed as a model of
cognition because it reduces capability of both handling very complex
problems and specializing in multiplicity. Attempts have not been made in
alignment with the goals of the knowledge network this proposal aims to
build, to design and augment learning mechanisms around \emph{contextual}
knowledge in a manner that reduces the intractability of hard problems and
is inter-operable between distinct learning mechanisms.

\begin{thebibliography}{}
  \bibitem{cancho03} i Cancho, R. F., \& Ricard V. Solé. (2003)
    Least effort and the origins of scaling in human language.
    In \emph{Proceedings of the National Academy of Sciences} 100(3), 788-791.
  \bibitem{zipf49} Zipf, G. K. (1949).
    Human Behaviour and the Principle of Least-Effort.
  \bibitem{barabasi99}
    Barab\'asi, A. L., \& Albert, R. (1999).
    Emergence of scaling in random networks.
    \emph{Science}, 286(5439), 509-512.
  \bibitem{clauset09} Clauset A., Shalizi, C. R., \& Newman M. E. J.  (2009)
    Power-law distributions in empirical data.
    \emph{SIAM Review} 51(4), 661-703 (arXiv:0706.1062, doi:10.1137/070710111)
  \bibitem{serre07}
    Serre, T., Oliva, A., \& Poggio, T. (2007).
    A feedforward architecture accounts for rapid categorization.
    In \emph{Proceedings of the National Academy of Sciences}, 104(15), 6424-6429.
  \bibitem{lake15} Lake, B. M., Salakhutdinov, R., \& Tenenbaum, J. B. (2015).
    Human-level concept learning through probabilistic program induction.
    \emph{Science}, 350(6266), 1332-1338.
  \bibitem{kulkarni15}
    Kulkarni, T. D., Whitney, W. F., Kohli, P., \& Tenenbaum, J. (2015).
    Deep convolutional inverse graphics network.
    In \emph{Advances in Neural Information Processing Systems} (pp. 2539-2547).
  \bibitem{mcdermott11}
    McDermott, J. H., \& Simoncelli, E. P. (2011).
    Sound texture perception via statistics of the auditory periphery:
    evidence from sound synthesis.
    \emph{Neuron}, 71(5), 926-940.
  \bibitem{tenenbaum01}
    Tenenbaum, J. B., \& Griffiths, T. L. (2001).
    The rational basis of representativeness.
    In \emph{Proceedings of the 23rd annual conference of the Cognitive
    Science Society} (p. 103641).
  \bibitem{liang10}
    Liang, P., Jordan, M. I., \& Klein, D. (2010).
    Learning programs: a hierarchical Bayesian approach.
    \emph{International Conference on Machine Learning}: 639–646
  \bibitem{dechter13}
    Dechter, E., Malmaud, J., Adams, R. P., \& Tenenbaum, J. B. (2013).
    Bootstrap Learning via Modular Concept Discovery.
    In \emph{IJCAI}.
  \bibitem{lavrac94}
    Lavrac, N., \& Dzeroski, S. (1994).
    Inductive Logic Programming.
    In \emph{WLP} (pp. 146-160).
  \bibitem{muggleton15}
    Muggleton, S. H., Lin, D., \& Tamaddoni-Nezhad, A. (2015).
    Meta-interpretive learning of higher-order dyadic datalog: Predicate
    invention revisited.
    \emph{Machine Learning}, 100(1), 49-73.
  \bibitem{goodfellow14}
    Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., ... \& Bengio, Y. (2014).
    Generative adversarial nets.
    In \emph{Advances in Neural Information Processing Systems} (pp.
    2672-2680).
\end{thebibliography}

\end{document}
